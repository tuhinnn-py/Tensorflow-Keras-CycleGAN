{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport h5py\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import load_img,img_to_array\npath='/kaggle/working'\npath_='/kaggle/input/horse2zebra/horse2zebra/'\ndef load_image(path,size=(256,256)):\n    dataset=list()\n    for filename in os.listdir(path):\n        file=os.path.join(path,filename)\n        img=load_img(file,size)\n        img_ar=img_to_array(img)\n        dataset.append(img_ar)\n    return np.asarray(dataset)\n\ntrainA=load_image(os.path.join(path_,'trainA'))\ntestA=load_image(os.path.join(path_,'testA'))\ntrainB=load_image(os.path.join(path_,'trainB'))\ntestB=load_image(os.path.join(path_,'testB'))\ntr_A=np.vstack((trainA,testA))\ntr_B=np.vstack((trainB,testB))\n\ndef load_images(X,Y):\n    X=(X-127.5)/127.5\n    Y=(Y-127.5)/127.5\n    return X,Y\ndataset=load_images(tr_A,tr_B)\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input,Conv2D,Conv2DTranspose,LeakyReLU,Add,Activation\nfrom tensorflow.keras.initializers import RandomNormal\nfrom keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom tensorflow.keras.optimizers import Adam\n\ndef define_discriminator(image_shape):\n    in_image=Input(image_shape)\n    init = RandomNormal(stddev=0.02)\n    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n    d = LeakyReLU(alpha=0.2)(d)\n\n    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n\n    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n\n    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n\n    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n    model = Model(in_image, patch_out)\n    model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5))#loss_weights=[0.5]\n    return model\n\ndef resnet_block(n_filters, input_layer):\n    init = RandomNormal(stddev=0.02)\n    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n\n    g = Add()([g, input_layer])\n    return g\n\ndef define_generator(image_shape, n_resnet=9):\n\tinit = RandomNormal(stddev=0.02)\n\tin_image = Input(shape=image_shape)\n\n\tg = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\n\tg = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\n\tg = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\n\tfor _ in range(n_resnet):\n\t\tg = resnet_block(256, g)\n\n\tg = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\n\tg = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\n\tg = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\tout_image = Activation('tanh')(g)\n\n\tmodel = Model(in_image, out_image)\n\treturn model\n\n\ndef define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n\tg_model_1.trainable = True\n\td_model.trainable = False\n\tg_model_2.trainable = False\n\n    #adversarial loss\n\tinput_gen = Input(shape=image_shape)\n\tgen1_out = g_model_1(input_gen)\n\toutput_d = d_model(gen1_out)\n\n    #forward cycle loss\n\tinput_id = Input(shape=image_shape)\n\toutput_id = g_model_1(input_id)#identity loss\n\toutput_f = g_model_2(gen1_out)\n\n\t# backward cycle loss\n\tgen2_out = g_model_2(input_id)\n\toutput_b = g_model_1(gen2_out)\n\n\tmodel = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n\topt = Adam(lr=0.0002, beta_1=0.5)\n\tmodel.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 10, 10, 10], optimizer=opt)\n\treturn model\n\nimport random\ndef generate_real_samples(dataset, n_samples, patch_shape):\n\tix = np.random.randint(0, dataset.shape[0], n_samples)\n\tX = dataset[ix]\n\ty = np.ones((n_samples, patch_shape, patch_shape, 1))\n\treturn X, y\n\ndef generate_fake_samples(g_model, dataset, patch_shape):\n\tX = g_model.predict(dataset)\n\ty = np.zeros((len(X), patch_shape, patch_shape, 1))\n\treturn X, y\n\ndef save_models(step, g_model_AtoB, g_model_BtoA):\n\t# save the first generator model\n\tfilename1 = os.path.join(path,'g_model_AtoB_%06d.h5' % (step+1))\n\tg_model_AtoB.save(filename1)\n\t# save the second generator model\n\tfilename2 = os.path.join(path,'g_model_BtoA_%06d.h5' % (step+1))\n\tg_model_BtoA.save(filename2)\n\tprint('>Saved: %s and %s' % (filename1, filename2))\n\nfrom matplotlib import pyplot\n# generate samples and save as a plot and save the model\ndef summarize_performance(step, g_model, trainX, name, n_samples=5):\n\t# select a sample of input images\n\tX_in, _ = generate_real_samples(trainX, n_samples, 0)\n\t# generate translated images\n\tX_out, _ = generate_fake_samples(g_model, X_in, 0)\n\t# scale all pixels from [-1,1] to [0,1]\n\tX_in = (X_in + 1) / 2.0\n\tX_out = (X_out + 1) / 2.0\n\t# plot real images\n\tfor i in range(n_samples):\n\t\tpyplot.subplot(2, n_samples, 1 + i)\n\t\tpyplot.axis('off')\n\t\tpyplot.imshow(X_in[i])\n\t# plot translated image\n\tfor i in range(n_samples):\n\t\tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n\t\tpyplot.axis('off')\n\t\tpyplot.imshow(X_out[i])\n\t# save plot to file\n\tfilename1 = os.path.join(path,'%s_generated_plot_%06d.png' % (name, (step+1)))\n\tpyplot.savefig(filename1)\n\tpyplot.close()\n\n# update image pool for fake images\ndef update_image_pool(pool, images, max_size=50):\n\tselected = list()\n\tfor image in images:\n\t\tif len(pool) < max_size:\n\t\t\t# stock the pool\n\t\t\tpool.append(image)\n\t\t\tselected.append(image)\n\t\telif random.random() < 0.5:\n\t\t\t# use image, but don't add it to the pool\n\t\t\tselected.append(image)\n\t\telse:\n\t\t\t# replace an existing image and use replaced image\n\t\t\tix = np.random.randint(0, len(pool))\n\t\t\tselected.append(pool[ix])\n\t\t\tpool[ix] = image\n\treturn np.asarray(selected)\n \n\ndef train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n\t# define properties of the training run\n\tn_epochs, n_batch, = 50, 1\n\t# determine the output square shape of the discriminator\n\tn_patch = d_model_A.output_shape[1]\n\t# unpack dataset\n\ttrainA, trainB = dataset\n\t# prepare image pool for fakes\n\tpoolA, poolB = list(), list()\n\t# calculate the number of batches per training epoch\n\tbat_per_epo = int(len(trainA) / n_batch)\n\t# calculate the number of training iterations\n\tn_steps = bat_per_epo * n_epochs\n\t# manually enumerate epochs\n\tfor i in range(n_steps):\n\t\t# select a batch of real samples\n\t\tX_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n\t\tX_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n\t\t# generate a batch of fake samples\n\t\tX_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n\t\tX_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n\t\t# update fakes from pool\n\t\tX_fakeA = update_image_pool(poolA, X_fakeA)\n\t\tX_fakeB = update_image_pool(poolB, X_fakeB)\n\t\t# update generator B->A via adversarial and cycle loss\n\t\tg_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n\t\t# update discriminator for A -> [real/fake]\n\t\tdA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n\t\tdA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n\t\t# update generator A->B via adversarial and cycle loss\n\t\tg_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n\t\t# update discriminator for B -> [real/fake]\n\t\tdB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n\t\tdB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n\t\t# summarize performance\n\t\tprint('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n\t\t# evaluate the model performance every so often\n\t\tif (i+1) % (bat_per_epo * 1) == 0:\n\t\t\t# plot A->B translation\n\t\t\tsummarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n\t\t\t# plot B->A translation\n\t\t\tsummarize_performance(i, g_model_BtoA, trainB, 'BtoA')\n\t\tif (i+1) % (bat_per_epo * 5) == 0:\n\t\t\t# save the models\n\t\t\tsave_models(i, g_model_AtoB, g_model_BtoA)\n\n# load image data\n#dataset =load_images(os.path.join(path,'horse2zebra_256.npz'))\nprint('Loaded', dataset[0].shape, dataset[1].shape)\nimage_shape = dataset[0].shape[1:]\nfrom tensorflow.keras.models import load_model\n#g_model_AtoB = define_generator(image_shape)\ng_model_AtoB=load_model('/kaggle/input/models/g_model_AtoB_035610.h5',custom_objects={'InstanceNormalization':keras_contrib.layers.InstanceNormalization})\ng_model_AtoB._name='htoz'\n#g_model_BtoA = define_generator(image_shape)\ng_model_BtoA=load_model('/kaggle/input/models/g_model_BtoA_035610.h5',custom_objects={'InstanceNormalization':keras_contrib.layers.InstanceNormalization})\ng_model_BtoA._name='ztoh'\nd_model_A = define_discriminator(image_shape)\nd_model_B = define_discriminator(image_shape)\nc_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\nc_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\ntrain(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)\n\n\n'''\npath='/kaggle/input/horse2zebra/horse2zebra/'\ndef load_image(path,size=(256,256)):\n    dataset=list()\n    for filename in os.listdir(path):\n        file=os.path.join(path,filename)\n        img=load_img(file,size)\n        img_ar=img_to_array(img)\n        dataset.append(img_ar)\n    return np.asarray(dataset)\n\ntrainA=load_image(os.path.join(path,'trainA'))\ntestA=load_image(os.path.join(path,'testA'))\ntrainB=load_image(os.path.join(path,'trainB'))\ntestB=load_image(os.path.join(path,'testB'))\ntr_A=np.vstack((trainA,testA))\ntr_B=np.vstack((trainB,testB))\nnp.savez_compressed(os.path.join('/kaggle/working','images'),tr_A,tr_B)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://www.github.com/keras-team/keras-contrib.git\nimport keras_contrib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_model_A.save('/kaggle/working/dA.h5')\nd_model_B.save('/kaggle/working/dB.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}